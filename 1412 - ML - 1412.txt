Several learning algorithms aim at discovering better representations of the inputs provided during training.[25] 
Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called 
representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way 
that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows 
reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to 
configurations that are implausible under that distribution. 
This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled 
input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In 
unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, 
independent component analysis, autoencoders, matrix factorization[26] and various forms of clustering.[27][28][29]
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional.